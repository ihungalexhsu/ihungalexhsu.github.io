---
title: "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation"
collection: publications
Authors: 'Kuan-Hao Huang, Varun Iyer, <b>I-Hung Hsu</b>, Anoop Kumar, Kai-Wei Chang, and Aram Galstyan.'
date: 07/2023
venue: 'ACL'
paperurl: 'https://aclanthology.org/2023.acl-long.447'
presentationurl: ../files/acl2023paraamr_slides.pdf
# codeurl: ''
excerpt: 'Area Chair Award'
---
---
<a href='https://aclanthology.org/2023.acl-long.447' target="_blank">[Download Paper]</a><a href='../../files/acl2023paraamr_poster.pdf' target="_blank">[Poster]</a><a href='../../files/acl2023paraamr_slides.pdf' target="_blank">[Slides]</a>

<p align="justify">
Paraphrase generation is a long-standing task in natural language processing (NLP). Supervised paraphrase generation models, which rely on human-annotated paraphrase pairs, are cost-inefficient and hard to scale up. On the other hand, automatically annotated paraphrase pairs (e.g., by machine back-translation), usually suffer from the lack of syntactic diversity â€“ the generated paraphrase sentences are very similar to the source sentences in terms of syntax. In this work, we present ParaAMR, a large-scale syntactically diverse paraphrase dataset created by abstract meaning representation back-translation. Our quantitative analysis, qualitative examples, and human evaluation demonstrate that the paraphrases of ParaAMR are syntactically more diverse compared to existing large-scale paraphrase datasets while preserving good semantic similarity. In addition, we show that ParaAMR can be used to improve on three NLP tasks: learning sentence embeddings, syntactically controlled paraphrase generation, and data augmentation for few-shot learning. Our results thus showcase the potential of ParaAMR for improving various NLP applications.
</p>